"use strict";(self.webpackChunkzeno_docs=self.webpackChunkzeno_docs||[]).push([[517],{5120:(t,e,n)=>{n.r(e),n.d(e,{assets:()=>r,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>l,toc:()=>u});var a=n(7462),s=(n(7294),n(3905));const i={},o="Instance views",l={unversionedId:"views/views",id:"views/views",title:"Instance views",description:"Zeno Instance Views are modular renderers for different data types and tasks.",source:"@site/docs/views/views.mdx",sourceDirName:"views",slug:"/views/",permalink:"/docs/views/",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Using Zeno",permalink:"/docs/using-zeno"}},r={},u=[],d={toc:u},c="wrapper";function p(t){let{components:e,...n}=t;return(0,s.kt)(c,(0,a.Z)({},d,n,{components:e,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"instance-views"},"Instance views"),(0,s.kt)("p",null,"Zeno Instance Views are modular renderers for different data types and tasks.\nEach of the following views can be passed as the ",(0,s.kt)("inlineCode",{parentName:"p"},"view")," option when creating a new project."),(0,s.kt)("table",null,(0,s.kt)("tr",null,(0,s.kt)("th",null,"View Name"),(0,s.kt)("th",null,"Description")),(0,s.kt)("tr",null,(0,s.kt)("td",null,"image-classification"),(0,s.kt)("td",null,"Display images with ground truth and predicted class labels. Works for both binary and multiclass classification. Requires image inputs and text or numeric outputs.")),(0,s.kt)("tr",null,(0,s.kt)("td",null,"text-classification"),(0,s.kt)("td",null,"Display text with ground truth and predicted class labels. Requires text inputs and text or numeric outputs.")),(0,s.kt)("tr",null,(0,s.kt)("td",null,"audio-transcription"),(0,s.kt)("td",null,"Display audio file along with outputed text, e.g. transcription. Requires audio inputs and text outputs.")),(0,s.kt)("tr",null,(0,s.kt)("td",null,"image-segmentation"),(0,s.kt)("td",null,"Display image with overlayed ground truth and predicted segmentation masks. Works for both binary segmentation. Requires image inputs and binary image outputs.")),(0,s.kt)("tr",null,(0,s.kt)("td",null,"code-generation"),(0,s.kt)("td",null,"Show formatted code input and code predictions. Use for evaluating code generation models such as Codex.")),(0,s.kt)("tr",null,(0,s.kt)("td",null,"openai-chat"),(0,s.kt)("td",null,"Show input-output pairs from chatbot models using the OpenAI API. See the"," ",(0,s.kt)("a",{href:"https://platform.openai.com/docs/guides/chat"},"API documentation")," ","for details on the required Chat data format.")),(0,s.kt)("tr",null,(0,s.kt)("td",null,"chatbot"),(0,s.kt)("td",null,"Show a single input-output pair from chatbot models.")),(0,s.kt)("tr",null,(0,s.kt)("td",null,"space-separated-values"),(0,s.kt)("td",null,"Table view of inputs, outputs, and labels which are space-separated words. Useful for tasks such as part-of-speech tagging."))))}p.isMDXComponent=!0}}]);