"use strict";(self.webpackChunkzeno_docs=self.webpackChunkzeno_docs||[]).push([[517],{5120:(t,e,n)=>{n.r(e),n.d(e,{assets:()=>u,contentTitle:()=>r,default:()=>k,frontMatter:()=>o,metadata:()=>l,toc:()=>d});var a=n(7462),s=(n(7294),n(3905)),i=n(614);const o={},r="Instance views",l={unversionedId:"views/views",id:"views/views",title:"Instance views",description:"Zeno Instance Views are modular renderers for different data types and tasks.",source:"@site/docs/views/views.mdx",sourceDirName:"views",slug:"/views/",permalink:"/docs/views/",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Using Zeno",permalink:"/docs/using-zeno"}},u={},d=[],c={toc:d},p="wrapper";function k(t){let{components:e,...n}=t;return(0,s.kt)(p,(0,a.Z)({},c,n,{components:e,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"instance-views"},"Instance views"),(0,s.kt)("p",null,"Zeno Instance Views are modular renderers for different data types and tasks.\nEach of the following views can be passed as the ",(0,s.kt)("inlineCode",{parentName:"p"},"view")," option when creating a new project."),(0,s.kt)("table",null,(0,s.kt)("tr",null,(0,s.kt)("th",null,"View Name"),(0,s.kt)("th",null,"Description")),(0,s.kt)("tr",null,(0,s.kt)("td",null,"image-classification"),(0,s.kt)("td",null,"Display images with ground truth and predicted class labels. Works for both binary and multiclass classification. Requires image inputs and text or numeric outputs.")),(0,s.kt)("tr",null,(0,s.kt)("td",null,"text-classification"),(0,s.kt)("td",null,"Display text with ground truth and predicted class labels. Requires text inputs and text or numeric outputs.")),(0,s.kt)("tr",null,(0,s.kt)("td",null,"audio-transcription"),(0,s.kt)("td",null,"Display audio file along with outputed text, e.g. transcription. Requires audio inputs and text outputs.")),(0,s.kt)("tr",null,(0,s.kt)("td",null,"image-segmentation"),(0,s.kt)("td",null,"Display image with overlayed ground truth and predicted segmentation masks. Works for both binary segmentation. Requires image inputs and binary image outputs.")),(0,s.kt)("tr",null,(0,s.kt)("td",null,"code-generation"),(0,s.kt)("td",null,"Show formatted code input and code predictions. Use for evaluating code generation models such as Codex.")),(0,s.kt)("tr",null,(0,s.kt)("td",null,"openai-chat"),(0,s.kt)("td",null,"Show input-output pairs from chatbot models using the OpenAI API. See the"," ",(0,s.kt)("a",{href:"https://platform.openai.com/docs/guides/chat"},"API documentation")," ","for details on the required Chat data format.")),(0,s.kt)("tr",null,(0,s.kt)("td",null,"openai-chat-markdown"),(0,s.kt)("td",null,"Same as `openai-chat` but renders each cell block as markdown. Useful for rendering complex interactions with external agents, etc.")),(0,s.kt)("tr",null,(0,s.kt)("td",null,"chatbot"),(0,s.kt)("td",null,"Show a single input-output pair from chatbot models.")),(0,s.kt)("tr",null,(0,s.kt)("td",null,"space-separated-values"),(0,s.kt)("td",null,"Table view of inputs, outputs, and labels which are space-separated words. Useful for tasks such as part-of-speech tagging.")),(0,s.kt)("tr",null,(0,s.kt)("td",null,"rag"),(0,s.kt)("td",null,"Show the input to a generator that can access information from other documents (retrieval augmented generation). The documents that are used and the system's answer can also be displayed.",(0,s.kt)("br",null),(0,s.kt)("br",null),"Input and label are just plain text whereas ouptut has to be JSON in the form:",(0,s.kt)(i.Z,{language:"json",mdxType:"CodeBlock"},'{\n    "retrieved": [\n        {\n            "reference": [YOUR_DOCUMENT],\n            "text": [TEXT_IN_YOUR_DOC],\n            "score": [RANKING_OF_DOC]\n        },\n        ...\n    ],\n    "answer": [SYSTEM_ANSWER],\n}\n')))))}k.isMDXComponent=!0}}]);