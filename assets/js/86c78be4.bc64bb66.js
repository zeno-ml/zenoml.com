"use strict";(self.webpackChunkzeno_docs=self.webpackChunkzeno_docs||[]).push([[3],{5705:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>o,metadata:()=>r,toc:()=>d});var a=t(5893),i=t(1151);const o={},s="Audio Transcription",r={id:"examples/transcription",title:"Audio Transcription",description:"Open with Zeno",source:"@site/docs/examples/transcription.mdx",sourceDirName:"examples",slug:"/examples/transcription",permalink:"/docs/examples/transcription",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"TaxEval",permalink:"/docs/examples/taxeval"},next:{title:"Integrations",permalink:"/docs/integrations/"}},l={},d=[{value:"Dependencies",id:"dependencies",level:2},{value:"Imports",id:"imports",level:2},{value:"Loading Metadata",id:"loading-metadata",level:3},{value:"Adding New Features",id:"adding-new-features",level:3},{value:"Create a Zeno Project",id:"create-a-zeno-project",level:3},{value:"Running Inference and Uploading Results",id:"running-inference-and-uploading-results",level:3},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",img:"img",p:"p",pre:"pre",...(0,i.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"audio-transcription",children:"Audio Transcription"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://hub.zenoml.com/project/62ec4e74-7358-4801-b80c-d19e51ff2a4f/Audio%20Transcription%20Accents",children:(0,a.jsx)(n.img,{src:"https://img.shields.io/badge/%20-Open_with_Zeno-612593.svg?labelColor=white&logo=data:image/svg%2bxml;base64,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",alt:"Open with Zeno"})}),"\n",(0,a.jsx)(n.a,{href:"https://github.com/zeno-ml/zeno-build/blob/main/transcription/transcription.ipynb",children:(0,a.jsx)(n.img,{src:"https://img.shields.io/badge/%20-Open_Notebook-F37726.svg?labelColor=white&logo=Jupyter",alt:"Open Notebook"})})]}),"\n",(0,a.jsxs)(n.p,{children:["Audio transcription is an essential task for applications such as voice assistants,\npodcast search, and video captioning. There are numerous open-source and commercial\ntools for audio transcription, and it can be difficult to know which one to use.\n",(0,a.jsx)(n.a,{href:"https://github.com/openai/whisper",children:"OpenAI's Whisper"})," API is often people's\ngo-to choice, but there are nine different models to choose from with different\nsizes, speeds, and cost."]}),"\n",(0,a.jsxs)(n.p,{children:["In this example, we'll use Zeno to compare the performance of the different\nmodels on the ",(0,a.jsx)(n.a,{href:"https://accent.gmu.edu/",children:"Speech Accent Archive"})," dataset.\nThe dataset has over 2,000 people from around the world reading the same\nparagraph in English. We'll use the dataset to evaluate the performance of\nthe different models on different accents and English fluency levels."]}),"\n",(0,a.jsx)(n.h2,{id:"dependencies",children:"Dependencies"}),"\n",(0,a.jsx)(n.p,{children:"Let's start by installing the required dependencies for this project:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pip install jiwer pandas openai-whisper zeno-client torch transformers tqdm\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Additionally, we'll need ffmpeg to run this example.\nYou can test if it is installed by running ",(0,a.jsx)(n.code,{children:"ffmpeg --help"}),".\nIf it is not found, you should install it through your package manager.\nFor example, if you are using conda, you can just run the following (and other managers such as brew and apt also work)."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"conda install ffmpeg\n"})}),"\n",(0,a.jsx)(n.h2,{id:"imports",children:"Imports"}),"\n",(0,a.jsx)(n.p,{children:"After this is all set up, we can now start running our analysis code and uploading data to Zeno.\nWe'll first import relevant libraries which we're going to use:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from jiwer import wer\nimport os\nimport pandas as pd\nimport whisper\nimport zeno_client\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nimport pandas as pd\nimport requests\nfrom io import BytesIO\nimport wave\nimport struct\nfrom tqdm import tqdm\n\ntqdm.pandas()\ndevice = "cuda:0" if torch.cuda.is_available() else "cpu"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n'})}),"\n",(0,a.jsx)(n.h3,{id:"loading-metadata",children:"Loading Metadata"}),"\n",(0,a.jsxs)(n.p,{children:["We'll be evaluating our model on raw audio, but will use a metadata file with additional information about each audio file ",(0,a.jsx)(n.a,{href:"https://github.com/zeno-ml/zeno-build/blob/main/transcription/speech_accent_archive.csv",children:"here"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'df = pd.read_csv("speech_accent_archive.csv")\ndf["data"] = "https://zenoml.s3.amazonaws.com/accents/" + df["id"]\n'})}),"\n",(0,a.jsx)(n.h3,{id:"adding-new-features",children:"Adding New Features"}),"\n",(0,a.jsxs)(n.p,{children:["In Zeno, we'll often want to enrich our metadata with some extra fields that might be useful for our analysis.\nIn this case, we are going to add the ",(0,a.jsx)(n.em,{children:"amplitude"})," and ",(0,a.jsx)(n.em,{children:"length"})," of the audio snippet as additional metadata fields:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Define the function to get amplitude and length\ndef get_amplitude_and_length_from_url(url):\n    # Download the WAV file content from the URL\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n\n        # Use the BytesIO object as input for the wave module\n        with wave.open(BytesIO(response.content), 'rb') as wav_file:\n            frame_rate = wav_file.getframerate()\n            n_frames = wav_file.getnframes()\n            n_channels = wav_file.getnchannels()\n            sample_width = wav_file.getsampwidth()\n            duration = n_frames / frame_rate\n\n            frames = wav_file.readframes(n_frames)\n            if sample_width == 1:  # 8-bit audio\n                fmt = '{}B'.format(n_frames * n_channels)\n            elif sample_width == 2:  # 16-bit audio\n                fmt = '{}h'.format(n_frames * n_channels)\n            else:\n                raise ValueError(\"Only supports up to 16-bit audio.\")\n\n            frame_amplitudes = struct.unpack(fmt, frames)\n            max_amplitude = max(frame_amplitudes)\n            max_amplitude_normalized = max_amplitude / float(int((2 ** (8 * sample_width)) / 2))\n\n            return max_amplitude_normalized, duration\n    except requests.RequestException as e:\n        print(f\"Request failed: {e}\")\n        return None, None\n\ndef apply_get_amplitude_and_length(row):\n    url = row['data']  # Assuming the URL is in the 'data' column\n    amplitude, length = get_amplitude_and_length_from_url(url)\n    return pd.Series({'amplitude': amplitude, 'length': length})\n\ndf[['amplitude', 'length']] = df.progress_apply(apply_get_amplitude_and_length, axis=1)\n"})}),"\n",(0,a.jsx)(n.h3,{id:"create-a-zeno-project",children:"Create a Zeno Project"}),"\n",(0,a.jsxs)(n.p,{children:["We can now upload our data to a Zeno project.\nYou will need your ",(0,a.jsx)(n.code,{children:"ZENO_API_KEY"})," here, which you can generate by clicking on your profile at the top right to navigate to your ",(0,a.jsx)(n.a,{href:"https://hub.zenoml.com/account",children:"account page"}),"."]}),"\n",(0,a.jsx)(n.p,{children:"Once you have your API key, you can authenticate with the Zeno client and create a project as follows:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'client = zeno_client.ZenoClient(YOUR_API_KEY)\n\nproject = client.create_project(\n    name="Audio Transcription Accents Evaluation",\n    view="audio-transcription",\n    description="Comparison of multiple audio transcription models",\n    metrics=[\n        zeno_client.ZenoMetric(name="avg wer", type="mean", columns=["wer"])\n    ]\n)\n'})}),"\n",(0,a.jsx)(n.p,{children:"We've already added a metric to our project that will help us track the average word error rate of different systems."}),"\n",(0,a.jsx)(n.p,{children:"You can click on the link provided in the output to start exploring your data!"}),"\n",(0,a.jsx)(n.h3,{id:"running-inference-and-uploading-results",children:"Running Inference and Uploading Results"}),"\n",(0,a.jsx)(n.p,{children:"We can already look at our data in Zeno, but would like to start evaluating the output transcriptions of models.\nLet's run inference for some of the popular OpenAI Whisper models."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Define what models to run inference on\nmodels = ["medium.en", "large-v1", "large-v2", "large-v3", "distil-medium.en", "distil-large-v2"]\nos.makedirs("cache", exist_ok=True)\n\n# Load inference data from cache or run inference for each model and add the data to a dataframe.\ndf_systems = []\nfor model_name in models:\n    try:\n        df_system = pd.read_parquet(f"cache/{model_name}.parquet")\n    except:\n        df_system = df[["id", "data", "label"]].copy()\n\n        if "distil" in model_name:\n            model_id = "distil-whisper/" + model_name\n            model = AutoModelForSpeechSeq2Seq.from_pretrained(\n                model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n            )\n            model.to(device)\n\n            processor = AutoProcessor.from_pretrained(model_id)\n            pipe = pipeline(\n                "automatic-speech-recognition",\n                model=model,\n                tokenizer=processor.tokenizer,\n                feature_extractor=processor.feature_extractor,\n                max_new_tokens=128,\n                chunk_length_s=15,\n                batch_size=16,\n                torch_dtype=torch_dtype,\n                device=device,\n            )\n            df_system["output"] = df_system["data"].progress_apply(lambda x: pipe(x)[\'text\'])\n            pass\n        else:\n            whisper_model = whisper.load_model(model_name)\n            df_system["output"] = df_system["data"].progress_apply(\n              lambda x: whisper_model.transcribe(x)["text"]\n            )\n\n        df_system["wer"] = df_system.progress_apply(lambda x: wer(x["label"], x["output"]), axis=1)\n        df_system.to_parquet(f"cache/{model_name}.parquet", index=False)\n    df_systems.append(df_system)\n'})}),"\n",(0,a.jsx)(n.p,{children:"You can see that we also calculate the word error rate (WER) for each model, a common metric for evaluating transcription models."}),"\n",(0,a.jsx)(n.p,{children:"We can now upload these results to our project:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'for i, df_system in enumerate(df_systems):\n    project.upload_system(\n      df_system[["id", "output", "wer"]], name=models[i], id_column="id", output_column="output"\n    )\n'})}),"\n",(0,a.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,a.jsxs)(n.p,{children:["If you've followed this example you should have a Zeno project similar to the one linked at the top of this page.\nLooking at ",(0,a.jsx)(n.a,{href:"https://hub.zenoml.com/project/62ec4e74-7358-4801-b80c-d19e51ff2a4f/Audio%20Transcription%20Accents/explore?params=eyJtb2RlbCI6IndoaXNwZXItYmFzZSIsIm1ldHJpYyI6eyJpZCI6NzYwLCJuYW1lIjoiYXZnX3dlciIsInR5cGUiOiJtZWFuIiwiY29sdW1ucyI6WyJ3ZXIiXX0sImNvbXBhcmlzb25Nb2RlbCI6InNpbGVyb19zc3QiLCJjb21wYXJpc29uQ29sdW1uIjp7ImlkIjoiNDhhYWM0ODgtNmU2Yi00YmNmLTkwYWYtMDUyMzM4OWI3YmNkIiwibmFtZSI6Im91dHB1dCIsImNvbHVtblR5cGUiOiJPVVRQVVQiLCJkYXRhVHlwZSI6Ik5PTUlOQUwiLCJtb2RlbCI6IndoaXNwZXItYmFzZSJ9LCJjb21wYXJlU29ydCI6W251bGwsdHJ1ZV0sIm1ldHJpY1JhbmdlIjpbMCwxLjYyMzE4ODQwNTc5NzEwMTZdLCJzZWxlY3Rpb25zIjp7InNsaWNlcyI6WzU2Ml0sIm1ldGFkYXRhIjp7fSwidGFncyI6W119fQ==",children:"high wer samples"})," by filtering for them using the histograms on the left is a great starting point to start finding the limitations of these state of the art models!"]})]})}function u(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},1151:(e,n,t)=>{t.d(n,{Z:()=>r,a:()=>s});var a=t(7294);const i={},o=a.createContext(i);function s(e){const n=a.useContext(o);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),a.createElement(o.Provider,{value:n},e.children)}}}]);